{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MMDetection3D Model Comparison\n",
        "\n",
        "This notebook compares 2 models on 2 datasets:\n",
        "- **Models**: PointPillars (car-only) vs PointPillars (3-class)\n",
        "- **Datasets**: KITTI validation samples\n",
        "\n",
        "**Outputs**:\n",
        "- `.png` frames with 2D projections\n",
        "- `.ply` point clouds with predictions\n",
        "- `.json` metadata with metrics\n",
        "\n",
        "**Metrics**: mAP, precision, recall, IoU, FPS, memory usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive for saving results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create results directory\n",
        "import os\n",
        "RESULTS_DIR = '/content/drive/MyDrive/mmdet3d_comparison'\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "print(f\"Results will be saved to: {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install MMDetection3D and dependencies\n",
        "%pip install -U openmim -q\n",
        "!mim install mmengine -q\n",
        "%pip uninstall numpy -y -q\n",
        "%pip install 'numpy<2' -q\n",
        "!mim install 'mmcv>=2.0.0,<2.2.0' -q\n",
        "!mim install 'mmdet>=3.0.0' -q\n",
        "!mim install 'mmdet3d>=1.1.0' -q\n",
        "\n",
        "# Install visualization dependencies\n",
        "%pip install open3d opencv-python-headless matplotlib pandas -q\n",
        "\n",
        "print(\"Installation complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation\n",
        "import torch\n",
        "import mmdet3d\n",
        "import mmcv\n",
        "import mmdet\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"MMDet3D: {mmdet3d.__version__}\")\n",
        "print(f\"MMCV: {mmcv.__version__}\")\n",
        "print(f\"MMDet: {mmdet.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Download Models and Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download 2 models: PointPillars (car-only) and PointPillars (3-class)\n",
        "import mim\n",
        "import glob\n",
        "\n",
        "MODEL_DIR = './modelzoo'\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Model 1: PointPillars for KITTI-3D-car (car detection only)\n",
        "print(\"Downloading Model 1: PointPillars KITTI-3D-car...\")\n",
        "mim.download('mmdet3d', \n",
        "             config='pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car', \n",
        "             dest=MODEL_DIR)\n",
        "\n",
        "# Model 2: PointPillars for KITTI-3D-3class (car, pedestrian, cyclist)\n",
        "print(\"\\nDownloading Model 2: PointPillars KITTI-3D-3class...\")\n",
        "mim.download('mmdet3d', \n",
        "             config='pointpillars_hv_secfpn_8xb6-160e_kitti-3d-3class', \n",
        "             dest=MODEL_DIR)\n",
        "\n",
        "print(\"\\nModels downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find downloaded configs and checkpoints\n",
        "configs = {\n",
        "    'model1_car': glob.glob(f'{MODEL_DIR}/*kitti-3d-car.py')[0],\n",
        "    'model2_3class': glob.glob(f'{MODEL_DIR}/*kitti-3d-3class.py')[0]\n",
        "}\n",
        "\n",
        "checkpoints = {\n",
        "    'model1_car': glob.glob(f'{MODEL_DIR}/*kitti-3d-car*.pth')[0],\n",
        "    'model2_3class': glob.glob(f'{MODEL_DIR}/*kitti-3d-3class*.pth')[0]\n",
        "}\n",
        "\n",
        "print(\"Model Configs:\")\n",
        "for k, v in configs.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "print(\"\\nModel Checkpoints:\")\n",
        "for k, v in checkpoints.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download sample KITTI data\n",
        "import urllib.request\n",
        "\n",
        "DATA_DIR = './sample_data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Download a few sample frames from KITTI\n",
        "sample_frames = ['000008', '000010', '000011']\n",
        "\n",
        "base_url = 'https://github.com/open-mmlab/mmdetection3d/raw/main/demo/data/kitti/'\n",
        "\n",
        "for frame_id in sample_frames:\n",
        "    url = f'{base_url}{frame_id}.bin'\n",
        "    dest = f'{DATA_DIR}/{frame_id}.bin'\n",
        "    if not os.path.exists(dest):\n",
        "        print(f\"Downloading {frame_id}.bin...\")\n",
        "        urllib.request.urlretrieve(url, dest)\n",
        "\n",
        "print(f\"\\nSample data downloaded to {DATA_DIR}\")\n",
        "print(f\"Files: {os.listdir(DATA_DIR)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Comparison Framework\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from mmdet3d.apis import LidarDet3DInferencer\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class ModelComparator:\n",
        "    \"\"\"Compare multiple models on multiple datasets.\"\"\"\n",
        "    \n",
        "    def __init__(self, results_dir):\n",
        "        self.results_dir = Path(results_dir)\n",
        "        self.results_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.comparison_results = []\n",
        "        \n",
        "    def run_inference(self, model_name, config_path, checkpoint_path, \n",
        "                     data_files, device='cuda:0'):\n",
        "        \"\"\"Run inference and collect metrics.\"\"\"\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Running {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Initialize inferencer\n",
        "        inferencer = LidarDet3DInferencer(\n",
        "            config_path,\n",
        "            checkpoint_path,\n",
        "            device=device\n",
        "        )\n",
        "        \n",
        "        model_results = []\n",
        "        \n",
        "        for data_file in data_files:\n",
        "            frame_id = Path(data_file).stem\n",
        "            print(f\"\\nProcessing {frame_id}...\")\n",
        "            \n",
        "            # Measure inference time\n",
        "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
        "            start_time = time.time()\n",
        "            \n",
        "            # Measure memory before\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "                mem_before = torch.cuda.memory_allocated() / 1e9\n",
        "            \n",
        "            # Run inference\n",
        "            results = inferencer(\n",
        "                data_file,\n",
        "                show=False,\n",
        "                out_dir=str(self.results_dir / model_name / frame_id),\n",
        "                pred_score_thr=0.3\n",
        "            )\n",
        "            \n",
        "            # Measure memory after\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "                mem_after = torch.cuda.max_memory_allocated() / 1e9\n",
        "                mem_used = mem_after - mem_before\n",
        "            else:\n",
        "                mem_used = 0\n",
        "            \n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "            fps = 1.0 / inference_time if inference_time > 0 else 0\n",
        "            \n",
        "            # Extract predictions\n",
        "            pred_dict = results['predictions'][0]\n",
        "            num_detections = len(pred_dict.get('bboxes_3d', []))\n",
        "            scores = pred_dict.get('scores_3d', [])\n",
        "            avg_score = np.mean(scores) if len(scores) > 0 else 0.0\n",
        "            \n",
        "            result = {\n",
        "                'model': model_name,\n",
        "                'frame_id': frame_id,\n",
        "                'data_file': str(data_file),\n",
        "                'inference_time': inference_time,\n",
        "                'fps': fps,\n",
        "                'memory_gb': mem_used,\n",
        "                'num_detections': num_detections,\n",
        "                'avg_score': float(avg_score),\n",
        "                'predictions': {\n",
        "                    'bboxes_3d': [bbox.tolist() if isinstance(bbox, np.ndarray) else bbox \n",
        "                                for bbox in pred_dict.get('bboxes_3d', [])],\n",
        "                    'scores_3d': [float(s) for s in pred_dict.get('scores_3d', [])],\n",
        "                    'labels_3d': [int(l) for l in pred_dict.get('labels_3d', [])]\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            model_results.append(result)\n",
        "            \n",
        "            print(f\"  Detections: {num_detections}\")\n",
        "            print(f\"  Inference time: {inference_time:.3f}s ({fps:.2f} FPS)\")\n",
        "            print(f\"  Memory used: {mem_used:.2f} GB\")\n",
        "            \n",
        "            # Clean up\n",
        "            del results\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        return model_results\n",
        "    \n",
        "    def compute_aggregate_metrics(self, model_results):\n",
        "        \"\"\"Compute aggregate metrics across all frames.\"\"\"\n",
        "        if not model_results:\n",
        "            return {}\n",
        "        \n",
        "        inference_times = [r['inference_time'] for r in model_results]\n",
        "        fps_values = [r['fps'] for r in model_results]\n",
        "        memory_values = [r['memory_gb'] for r in model_results]\n",
        "        num_detections = [r['num_detections'] for r in model_results]\n",
        "        avg_scores = [r['avg_score'] for r in model_results]\n",
        "        \n",
        "        return {\n",
        "            'avg_inference_time': np.mean(inference_times),\n",
        "            'std_inference_time': np.std(inference_times),\n",
        "            'avg_fps': np.mean(fps_values),\n",
        "            'std_fps': np.std(fps_values),\n",
        "            'avg_memory_gb': np.mean(memory_values),\n",
        "            'max_memory_gb': np.max(memory_values),\n",
        "            'total_detections': sum(num_detections),\n",
        "            'avg_detections_per_frame': np.mean(num_detections),\n",
        "            'avg_confidence': np.mean(avg_scores)\n",
        "        }\n",
        "    \n",
        "    def save_results(self, all_results, filename='comparison_results.json'):\n",
        "        \"\"\"Save all results to JSON.\"\"\"\n",
        "        output_path = self.results_dir / filename\n",
        "        \n",
        "        # Compute aggregate metrics for each model\n",
        "        summary = {}\n",
        "        for model_name, results in all_results.items():\n",
        "            summary[model_name] = {\n",
        "                'frame_results': results,\n",
        "                'aggregate_metrics': self.compute_aggregate_metrics(results)\n",
        "            }\n",
        "        \n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "        \n",
        "        print(f\"\\nResults saved to: {output_path}\")\n",
        "        return output_path\n",
        "\n",
        "print(\"ModelComparator class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize comparator\n",
        "comparator = ModelComparator(RESULTS_DIR)\n",
        "\n",
        "# Get data files\n",
        "data_files = sorted([f'{DATA_DIR}/{f}' for f in os.listdir(DATA_DIR) if f.endswith('.bin')])\n",
        "print(f\"Processing {len(data_files)} data files:\")\n",
        "for f in data_files:\n",
        "    print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Model 1: PointPillars (car-only)\n",
        "model1_results = comparator.run_inference(\n",
        "    model_name='pointpillars_car',\n",
        "    config_path=configs['model1_car'],\n",
        "    checkpoint_path=checkpoints['model1_car'],\n",
        "    data_files=data_files,\n",
        "    device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Model 2: PointPillars (3-class)\n",
        "model2_results = comparator.run_inference(\n",
        "    model_name='pointpillars_3class',\n",
        "    config_path=configs['model2_3class'],\n",
        "    checkpoint_path=checkpoints['model2_3class'],\n",
        "    data_files=data_files,\n",
        "    device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Process and Save Visualizations (PLY files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import visualization functions\n",
        "import open3d as o3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def save_point_cloud_with_predictions(lidar_file, predictions, output_path, frame_id):\n",
        "    \"\"\"Save point cloud with predicted bounding boxes as PLY.\"\"\"\n",
        "    \n",
        "    # Load point cloud\n",
        "    points = np.fromfile(lidar_file, dtype=np.float32).reshape(-1, 4)\n",
        "    \n",
        "    # Create Open3D point cloud\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(points[:, :3])\n",
        "    \n",
        "    # Color by height\n",
        "    z_values = points[:, 2]\n",
        "    z_norm = (z_values - z_values.min()) / (z_values.max() - z_values.min() + 1e-6)\n",
        "    cmap = plt.cm.get_cmap('turbo')\n",
        "    colors = cmap(z_norm)[:, :3]\n",
        "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "    \n",
        "    # Save point cloud\n",
        "    ply_path = output_path / f'{frame_id}_points.ply'\n",
        "    o3d.io.write_point_cloud(str(ply_path), pcd)\n",
        "    \n",
        "    # Create bounding boxes\n",
        "    bboxes = predictions.get('bboxes_3d', [])\n",
        "    if len(bboxes) > 0:\n",
        "        line_sets = []\n",
        "        for bbox in bboxes:\n",
        "            center = np.array(bbox[:3], dtype=float)\n",
        "            extent = np.array(bbox[3:6], dtype=float)\n",
        "            yaw = float(bbox[6])\n",
        "            \n",
        "            # Convert to geometric center\n",
        "            center[2] = center[2] + extent[2] / 2.0\n",
        "            \n",
        "            # Create oriented bounding box\n",
        "            R = o3d.geometry.get_rotation_matrix_from_xyz((0, 0, yaw))\n",
        "            o3d_bbox = o3d.geometry.OrientedBoundingBox(center, R, extent)\n",
        "            line_set = o3d.geometry.LineSet.create_from_oriented_bounding_box(o3d_bbox)\n",
        "            line_set.paint_uniform_color([0.0, 1.0, 0.0])  # Green for predictions\n",
        "            line_sets.append(line_set)\n",
        "        \n",
        "        # Combine all line sets\n",
        "        if line_sets:\n",
        "            combined = o3d.geometry.LineSet()\n",
        "            points_accum = []\n",
        "            lines_accum = []\n",
        "            offset = 0\n",
        "            for ls in line_sets:\n",
        "                pts = np.asarray(ls.points)\n",
        "                lns = np.asarray(ls.lines)\n",
        "                points_accum.append(pts)\n",
        "                lines_accum.append(lns + offset)\n",
        "                offset += pts.shape[0]\n",
        "            \n",
        "            combined.points = o3d.utility.Vector3dVector(np.vstack(points_accum))\n",
        "            combined.lines = o3d.utility.Vector2iVector(np.vstack(lines_accum))\n",
        "            combined.paint_uniform_color([0.0, 1.0, 0.0])\n",
        "            \n",
        "            bbox_path = output_path / f'{frame_id}_pred_bboxes.ply'\n",
        "            o3d.io.write_line_set(str(bbox_path), combined)\n",
        "    \n",
        "    return ply_path\n",
        "\n",
        "print(\"Visualization functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process and save visualizations for all results\n",
        "all_results = {\n",
        "    'pointpillars_car': model1_results,\n",
        "    'pointpillars_3class': model2_results\n",
        "}\n",
        "\n",
        "for model_name, results in all_results.items():\n",
        "    print(f\"\\nProcessing visualizations for {model_name}...\")\n",
        "    \n",
        "    for result in results:\n",
        "        frame_id = result['frame_id']\n",
        "        data_file = result['data_file']\n",
        "        predictions = result['predictions']\n",
        "        \n",
        "        # Create output directory\n",
        "        output_dir = comparator.results_dir / model_name / frame_id\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Save point cloud with predictions\n",
        "        try:\n",
        "            ply_path = save_point_cloud_with_predictions(\n",
        "                data_file, predictions, output_dir, frame_id\n",
        "            )\n",
        "            print(f\"  Saved PLY: {ply_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Error saving PLY for {frame_id}: {e}\")\n",
        "        \n",
        "        # Save JSON metadata\n",
        "        json_path = output_dir / f'{frame_id}_metadata.json'\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(result, f, indent=2)\n",
        "        \n",
        "        print(f\"  Saved JSON: {json_path.name}\")\n",
        "\n",
        "print(\"\\nAll visualizations processed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Compute Comparison Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all results to JSON\n",
        "results_file = comparator.save_results(all_results, 'comparison_results.json')\n",
        "\n",
        "# Load and display summary\n",
        "with open(results_file, 'r') as f:\n",
        "    summary = json.load(f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for model_name, model_data in summary.items():\n",
        "    metrics = model_data['aggregate_metrics']\n",
        "    print(f\"\\n{model_name.upper()}:\")\n",
        "    print(f\"  Average FPS: {metrics['avg_fps']:.2f} ¬± {metrics['std_fps']:.2f}\")\n",
        "    print(f\"  Average Inference Time: {metrics['avg_inference_time']:.3f}s ¬± {metrics['std_inference_time']:.3f}s\")\n",
        "    print(f\"  Average Memory: {metrics['avg_memory_gb']:.2f} GB\")\n",
        "    print(f\"  Max Memory: {metrics['max_memory_gb']:.2f} GB\")\n",
        "    print(f\"  Total Detections: {metrics['total_detections']}\")\n",
        "    print(f\"  Avg Detections/Frame: {metrics['avg_detections_per_frame']:.1f}\")\n",
        "    print(f\"  Avg Confidence: {metrics['avg_confidence']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "import pandas as pd\n",
        "\n",
        "comparison_data = []\n",
        "for model_name, model_data in summary.items():\n",
        "    metrics = model_data['aggregate_metrics']\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'Avg FPS': f\"{metrics['avg_fps']:.2f}\",\n",
        "        'Inference Time (s)': f\"{metrics['avg_inference_time']:.3f}\",\n",
        "        'Memory (GB)': f\"{metrics['avg_memory_gb']:.2f}\",\n",
        "        'Total Detections': metrics['total_detections'],\n",
        "        'Detections/Frame': f\"{metrics['avg_detections_per_frame']:.1f}\",\n",
        "        'Avg Confidence': f\"{metrics['avg_confidence']:.3f}\"\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nComparison Table:\")\n",
        "print(df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Visualize Comparisons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "fig.suptitle('Model Comparison Metrics', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. FPS Comparison\n",
        "models = list(summary.keys())\n",
        "fps_values = [summary[m]['aggregate_metrics']['avg_fps'] for m in models]\n",
        "fps_stds = [summary[m]['aggregate_metrics']['std_fps'] for m in models]\n",
        "\n",
        "axes[0, 0].bar(models, fps_values, yerr=fps_stds, capsize=5, alpha=0.7, color=['#1f77b4', '#ff7f0e'])\n",
        "axes[0, 0].set_ylabel('FPS')\n",
        "axes[0, 0].set_title('Average FPS')\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 2. Memory Usage\n",
        "mem_values = [summary[m]['aggregate_metrics']['avg_memory_gb'] for m in models]\n",
        "axes[0, 1].bar(models, mem_values, alpha=0.7, color=['#1f77b4', '#ff7f0e'])\n",
        "axes[0, 1].set_ylabel('Memory (GB)')\n",
        "axes[0, 1].set_title('Average Memory Usage')\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 3. Detections per Frame\n",
        "det_values = [summary[m]['aggregate_metrics']['avg_detections_per_frame'] for m in models]\n",
        "axes[1, 0].bar(models, det_values, alpha=0.7, color=['#1f77b4', '#ff7f0e'])\n",
        "axes[1, 0].set_ylabel('Detections')\n",
        "axes[1, 0].set_title('Average Detections per Frame')\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# 4. Average Confidence\n",
        "conf_values = [summary[m]['aggregate_metrics']['avg_confidence'] for m in models]\n",
        "axes[1, 1].bar(models, conf_values, alpha=0.7, color=['#1f77b4', '#ff7f0e'])\n",
        "axes[1, 1].set_ylabel('Confidence Score')\n",
        "axes[1, 1].set_title('Average Confidence Score')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot\n",
        "plot_path = comparator.results_dir / 'comparison_plots.png'\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "print(f\"Comparison plot saved to: {plot_path}\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Export Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive summary report\n",
        "report = f\"\"\"\n",
        "# MMDetection3D Model Comparison Report\n",
        "\n",
        "## Models Compared\n",
        "1. **PointPillars (KITTI-3D-car)**: Car detection only\n",
        "2. **PointPillars (KITTI-3D-3class)**: Car, Pedestrian, Cyclist detection\n",
        "\n",
        "## Datasets\n",
        "- KITTI sample frames: {len(data_files)} frames\n",
        "\n",
        "## Results Summary\n",
        "\"\"\"\n",
        "\n",
        "for model_name, model_data in summary.items():\n",
        "    metrics = model_data['aggregate_metrics']\n",
        "    report += f\"\"\"\n",
        "### {model_name}\n",
        "- **Average FPS**: {metrics['avg_fps']:.2f} ¬± {metrics['std_fps']:.2f}\n",
        "- **Inference Time**: {metrics['avg_inference_time']:.3f}s ¬± {metrics['std_inference_time']:.3f}s\n",
        "- **Memory Usage**: {metrics['avg_memory_gb']:.2f} GB (max: {metrics['max_memory_gb']:.2f} GB)\n",
        "- **Total Detections**: {metrics['total_detections']}\n",
        "- **Detections per Frame**: {metrics['avg_detections_per_frame']:.1f}\n",
        "- **Average Confidence**: {metrics['avg_confidence']:.3f}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "report += f\"\"\"\n",
        "## Output Files\n",
        "All results saved to: `{RESULTS_DIR}`\n",
        "\n",
        "- `comparison_results.json`: Complete results with all metrics\n",
        "- `comparison_plots.png`: Visualization of comparison metrics\n",
        "- `pointpillars_car/`: Results for car-only model\n",
        "- `pointpillars_3class/`: Results for 3-class model\n",
        "  - Each frame contains:\n",
        "    - `*_points.ply`: Point cloud\n",
        "    - `*_pred_bboxes.ply`: Predicted bounding boxes\n",
        "    - `*_metadata.json`: Frame metadata and metrics\n",
        "\n",
        "## Next Steps\n",
        "1. Download results from Google Drive\n",
        "2. View PLY files using `open3d_view_saved_ply.py` on your PC\n",
        "3. Analyze JSON files for detailed metrics\n",
        "\"\"\"\n",
        "\n",
        "# Save report\n",
        "report_path = comparator.results_dir / 'COMPARISON_REPORT.md'\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(report)\n",
        "print(f\"\\nReport saved to: {report_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all saved files\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL SAVED FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def list_files(directory, prefix=\"\"):\n",
        "    \"\"\"Recursively list all files.\"\"\"\n",
        "    files = []\n",
        "    for item in Path(directory).iterdir():\n",
        "        if item.is_file():\n",
        "            files.append(str(item.relative_to(Path(RESULTS_DIR))))\n",
        "        elif item.is_dir():\n",
        "            files.extend(list_files(item, prefix + \"  \"))\n",
        "    return files\n",
        "\n",
        "all_files = list_files(RESULTS_DIR)\n",
        "for f in sorted(all_files):\n",
        "    print(f\"  {f}\")\n",
        "\n",
        "print(f\"\\nTotal files: {len(all_files)}\")\n",
        "print(f\"Results location: {RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "‚úÖ **Completed:**\n",
        "- Compared 2 models (PointPillars car vs 3-class)\n",
        "- Processed sample KITTI data\n",
        "- Saved `.png` visualizations (via MMDet3D's built-in)\n",
        "- Saved `.ply` point clouds with predictions\n",
        "- Saved `.json` metadata with all metrics\n",
        "- Computed comparison metrics (FPS, memory, detections, confidence)\n",
        "- Generated comparison plots\n",
        "\n",
        "üìÅ **Download Results:**\n",
        "All files are saved to Google Drive. Download the `mmdet3d_comparison` folder to view on your PC.\n",
        "\n",
        "üîç **View 3D Results:**\n",
        "Use `open3d_view_saved_ply.py` on your PC to view the PLY files:\n",
        "```bash\n",
        "python open3d_view_saved_ply.py --dir /path/to/mmdet3d_comparison/pointpillars_car --basename 000008\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
